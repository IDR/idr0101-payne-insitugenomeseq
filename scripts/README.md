README
------

This describes the conversion pipeline

Requirements
------------

-   Docker
-   the Docker image built from the Dockerfile in this repository which 
    includes the bioformats2raw and raw2ometiff utilities.
-   a Python virtual environment with awscli installed for the S3 upload

Note: the bioformat2raw feature allowing to export only the first series of
the source image is required for this pipeline.

Sources
-------

As the original files are Imaris IMS files, one per timepoint, a directory
called 20201215-patterns contains symlinks to the original data and pattern
files allowing to combine the individual cycles into a timelapse
representation.

Conversion scripts
------------------

The conversion pipeline consists of three scripts:

-   [convert.sh](convert.sh) converts the original IMS files into OME-Zarr and
    OME-TIFF. It executes `bioformat2raw` then `raw2ometiff` against the
    pattern file describing the sequencing acquisition and the two standalone
    IMS files. Data is generated in 2 folders `YYYYMMDD-zarr` and
    `YYYYMMDD-ometiff` where `YYYYMMDD` is the date.
-   [s3-upload.sh](s3-upload.sh) uploads the OME-Zarr filesets to S3. It
    renames the `0` group under `data.zarr` for each of the 3 Zarr datasets
    generated by `convert.sh` and uploads it to the S3 using `awscli`
-   [idrftp-aspera.sh](idrftp-aspera.sh) copies the OME-TIFF files to NFS via
    Aspera

Parallel conversion
-------------------

To speed up the conversion, the execution of the scripts above has been 
executed in parallel using the GNU `parallel` utility.

The sequence was tested against the first four embryos and then applied to the
57 embryo datasets.

The parallel conversion took ~6h for four embryos (generating ~800GB of data):

```
$ parallel --joblog $(date '+%Y%m%d')_convert.log --results $(date '+%Y%m%d')_convert ./convert.sh ::: embryo01 embryo02 embryo03 embryo04
$ cat $(date '+%Y%m%d')_convert.log
Seq     Host    Starttime       JobRuntime      Send    Receive Exitval Signal  Command
1       :       1608031486.525   15924.329      0       126441605       0       0       ./convert.sh embryo01
2       :       1608031486.528   18674.967      0       162863159       0       0       ./convert.sh embryo02
3       :       1608031486.531   20964.452      0       162886029       0       0       ./convert.sh embryo03
4       :       1608031486.534   21092.129      0       162885643       0       0       ./convert.sh embryo04
```

The S3 upload typically takes 1-2h although the transfer rate decreases with the number of concurrent uploads:

```
$ parallel --joblog $(date '+%Y%m%d')_upload.log --results $(date '+%Y%m%d')_upload ./s3-upload.sh ::: embryo01 embryo02 embryo03 embryo04
$ cat $(date '+%Y%m%d')_upload.log
Seq     Host    Starttime       JobRuntime      Send    Receive Exitval Signal  Command
3       :       1608124765.920    2175.603      0       89010431        0       0       DATE=20201215 ./s3-upload.sh embryo04
2       :       1608124765.916    4170.486      0       88970953        0       0       DATE=20201215 ./s3-upload.sh embryo03
1       :       1608124765.913    4452.721      0       89148788        0       0       DATE=20201215 ./s3-upload.sh embryo02
````

Before copying the data to Aspera, the Zarr folder should be removed to avoid
unnecessary data duplication between NFS and S3:

```
rm -rf /data/idr0101-payne-insitugenomeseq/*-zarr/
```

Finally the OME-TIFF can be transferred over via Aspera and the folder removed
to start the conversion of the next batch of data:

```
sudo ASPERA_SCP_PASS=<PASS> ./idrftp-aspera.sh /data/idr0101-payne-insitugenomeseq/
rm -rf /data/idr0101-payne-insitugenomeseq/*-ometiff/
```

The [pipeline.sh](pipeline.sh) script wraps the steps described above and was
executed by passing typically four embryos at once:

```
./pipeline.sh embryo01 embryo02 embryo03 embryo04
```

Raw data import
---------------

Experiment A only needs 2 raw data files: `pgp1f_hyb.nd2` and `pgp1f.pattern`, which are
imported as many images in a single Dataset: `Fibroblasts_01`.
Images are named `pgp1f [pgp1f_cycle01.nd2 (series 01)]` and `pgp1f_hyb [pgp1f_hyb.nd2 (series 01)]`.
We create new Datasets `Fibroblasts_nn` and place the images from the corresponding series
into each using:

```
python scripts/post_import_expA.py
```

Experiment B raw data is imported as normal.

Processed data
--------------

Processed data is within `20210421-ftp`.
Each image like `cell001/cy01_ch01.tif` is a Z-stack at a particular channel and cycle (timepoint).
To create multi-dimensional images from these, we create symlinks from e.g. `cell01_t01_c01.tif`
and a corresponding pattern file.
These will all be created within a sibling directory `20210421-processed` using a script:

```
cd /uod/idr/filesets/idr0101-payne-insitugenomeseq/
/uod/idr/metadata/idr0101-payne-insitugenomeseq/scripts/processed_symlinks.sh
```

The script also creates a `20210421-processed/processed-filePaths.tsv` file for bulk
import of the pattern files.
Currently, this needs a manually-created `bulk.yml` file placed in the same directory:

```
---
path: "processed-filePaths.tsv"
continue: true
transfer: "ln_s"
exclude: "clientpath"
skip:
  - checksum
  - minmax
columns:
  - target
  - path
  - name
  ```

Create ROIs from CSV data_tables
--------------------------------

For Embryos in ExperimentB, we have `data_table.csv` files with each row representing
a data measurement coming from a Point on the processed images.
To create ROIs and Points on the processed images, we can run

```
# uses cli login
python /uod/idr/metadata/idr0101-payne-insitugenomeseq/scripts/csv_to_points.py
```

For each processed Image such as `cell01_processed` in each Dataset of the `experimentB` Project, the script
will open the corresponding CSV file, e.g:

```
20210421-ftp/annotations/embryo/data_tables/embryo01_data_table.csv
```

and filter the rows by the Cell ID, then create a Point for each, grouped into an ROI per chromosome.
